{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aithu-Snehith/testing-colab/blob/master/mnist_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2NBtzIzgFylM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsFQQQfbF8_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "93fecaa4-2acb-4f92-f0a9-90b09c1a478c"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZ8AiSa1H1HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = tf.placeholder('float')\n",
        "labels = tf.placeholder('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUZrvX0FIIHI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(data):\n",
        "  layer_1 = {'weights':tf.Variable(tf.random_normal([784,600])), 'biases':tf.random_normal([600])}\n",
        "  layer_2 = {'weights':tf.Variable(tf.random_normal([600,512])), 'biases':tf.random_normal([512])}\n",
        "  layer_3 = {'weights':tf.Variable(tf.random_normal([512,256])), 'biases':tf.random_normal([256])}\n",
        "  layer_4 = {'weights':tf.Variable(tf.random_normal([256,10])), 'biases':tf.random_normal([10])}\n",
        "  \n",
        "  net = tf.add(tf.matmul(data, layer_1['weights']),layer_1['biases'])\n",
        "  net = tf.nn.relu(net)\n",
        "  net = tf.add(tf.matmul(net, layer_2['weights']),layer_2['biases'])\n",
        "  net = tf.nn.relu(net)\n",
        "  net = tf.add(tf.matmul(net, layer_3['weights']),layer_3['biases'])\n",
        "  net = tf.nn.relu(net)\n",
        "  net = tf.add(tf.matmul(net, layer_4['weights']),layer_4['biases'])\n",
        "  \n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZQGo0THJyDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inputs):\n",
        "  predicted = network(inputs)\n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = predicted, labels = labels))\n",
        "  optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "  \n",
        "  epochs = 10\n",
        "  batch_size = 128\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      loss_epoch = 0\n",
        "      for i in range(int(mnist.train.num_examples/batch_size)):\n",
        "        temp_x, temp_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        _,cost = sess.run([optimizer, loss], feed_dict = {inputs: temp_x , labels: temp_y})\n",
        "        loss_epoch = loss_epoch + cost\n",
        "      \n",
        "      print('epoch: '+ str(epoch) + '/' + str(epochs) + '  loss: ' +  str(loss_epoch))\n",
        "\n",
        "    correct = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels, 1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "    print('Accuracy:',accuracy.eval({inputs:mnist.test.images, labels:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZBT46ENMDXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e8360526-4e24-44c1-b7e6-14f5ba1f7f6f"
      },
      "cell_type": "code",
      "source": [
        "train(inputs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-e833ba28b84f>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "epoch: 0/10  loss: 1231074.798828125\n",
            "epoch: 1/10  loss: 295179.74838256836\n",
            "epoch: 2/10  loss: 174753.00561523438\n",
            "epoch: 3/10  loss: 114765.04658508301\n",
            "epoch: 4/10  loss: 79288.73290634155\n",
            "epoch: 5/10  loss: 56642.86838340759\n",
            "epoch: 6/10  loss: 41556.36660827532\n",
            "epoch: 7/10  loss: 30871.450489997864\n",
            "epoch: 8/10  loss: 23690.019052047\n",
            "epoch: 9/10  loss: 18018.513149261475\n",
            "('Accuracy:', 0.9488)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}